# 🧠 LLM 服务模块

## 📘 概述

LLM 服务模块是 relaScope-insight 应用中集成和管理大语言模型服务的核心组件。该模块负责处理与各种 LLM 提供商的 API 交互，管理模型配置，处理聊天会话，并将 LLM 能力提供给应用的其他部分使用。

## 🏗️ 目录结构

```
/llm/
├── config/            # LLM 配置管理
├── prompts/           # 提示词模板
├── utils/             # 工具函数
├── config-manager.ts  # 配置管理器实现
├── llm-service.ts     # LLM 服务主实现
└── types.ts           # 类型定义
```

## 🚀 核心功能

- 🔌 多 LLM 提供商集成 (OpenAI, Anthropic, Gemini 等)
- ⚙️ 模型配置与参数管理
- 🔑 API 密钥安全管理
- 🌐 请求代理支持
- 💬 聊天上下文处理
- 📋 提示词模板管理
- 🔄 流式输出支持

## 🔍 主要组件

### LLM 服务 (llm-service.ts)
- 封装各种 LLM 提供商的 API 调用
- 处理聊天请求和响应
- 支持流式响应处理
- 实现错误处理和重试逻辑
- 提供统一的服务接口

### 配置管理器 (config-manager.ts)
- 管理 LLM 提供商配置
- 存储和检索模型设置
- API 密钥管理
- 代理设置处理

### 类型定义 (types.ts)
- 定义 LLM 交互相关的数据结构
- 提供模型和提供商的类型信息
- 会话和消息的类型规范

### 提示词模板 (prompts/)
- 预定义提示词模板
- 支持模板参数化
- 提供特定任务的优化提示

## 💡 使用流程

1. 从配置管理器加载 LLM 提供商设置
2. 用户选择特定配置和模型
3. LLM 服务使用相应配置初始化客户端
4. 处理用户输入，可能应用提示词模板
5. 发送请求到 LLM 提供商 API
6. 接收并处理 API 响应
7. 将结果返回给调用方

## 🛠️ 主要 API

- `sendMessage()` - 发送消息到 LLM 并获取响应
- `streamMessage()` - 以流式方式获取 LLM 响应
- `getProviders()` - 获取可用的 LLM 提供商列表
- `getModels()` - 获取特定提供商的可用模型
- `saveConfig()` - 保存 LLM 配置
- `getConfig()` - 检索 LLM 配置

## 🔌 依赖关系

- 依赖配置服务存储配置
- 依赖安全服务处理 API 密钥
- 通过 IPC 向渲染进程提供功能
- 可能需要网络服务进行代理或连接管理 